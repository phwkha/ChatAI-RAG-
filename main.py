import os
import time

OLLAMA_URL = os.getenv("OLLAMA_HOST", "http://localhost:11434")

print(f"ğŸ”Œ Äang káº¿t ná»‘i tá»›i mÃ¡y AI táº¡i: {OLLAMA_URL}")

# Import cÃ¡c cÃ´ng cá»¥
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

def main():
    print("â³ Äang chá» mÃ¡y AI khá»Ÿi Ä‘á»™ng (5 giÃ¢y)...")
    time.sleep(5) 

    # 1. Äá»ŒC FILE
    print("ğŸ“‚ Äang Ä‘á»c tÃ i liá»‡u data.txt...")
    try:
        loader = TextLoader("data.txt", encoding='utf-8')
        docs = loader.load()
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file: {e}")
        return

    # 2. Cáº®T NHá» & MÃƒ HÃ“A
    print("âœ‚ï¸  Äang xá»­ lÃ½ dá»¯ liá»‡u...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    print(f"\nğŸ“Š BÃO CÃO Dá»® LIá»†U:")
    print(f"   - Tá»•ng sá»‘ Ä‘oáº¡n vÄƒn Ä‘Ã£ cáº¯t: {len(splits)} Ä‘oáº¡n")
    if len(splits) > 0:
        print(f"   - Ná»™i dung Ä‘oáº¡n Ä‘áº§u tiÃªn AI Ä‘á»c Ä‘Æ°á»£c lÃ :")
        print(f"     \"{splits[0].page_content[:100]}...\"")
        print("--------------------------------------------------\n")
    else:
        print("âš ï¸ Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u nÃ o! HÃ£y kiá»ƒm tra file data.txt")

    # 3. Táº O Bá»˜ NHá»š VECTOR
    embeddings = OllamaEmbeddings(
        model="nomic-embed-text",
        base_url=OLLAMA_URL
    )
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

    # 4. KHá»I Táº O DEEPSEEK
    print("ğŸ¤– Äang káº¿t ná»‘i vá»›i DeepSeek...")
    llm = ChatOllama(
        model="deepseek-r1:8b",
        base_url=OLLAMA_URL,
        temperature=0.3
    )

    # 5. Táº O Há»˜I THOáº I
    system_prompt = (
        "Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»¯u Ã­ch vÃ  trung thá»±c. "
        "Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tá»•ng há»£p vÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin trong vÄƒn báº£n (Context) bÃªn dÆ°á»›i.\n"
        "YÃŠU Cáº¦U QUAN TRá»ŒNG:\n"
        "1. Tráº£ lá»i báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn, máº¡ch láº¡c, dá»… hiá»ƒu (khÃ´ng liá»‡t kÃª mÃ¡y mÃ³c).\n"
        "2. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong Context. Náº¿u Context khÃ´ng nháº¯c Ä‘áº¿n, tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘Æ°á»£c tá»± bá»‹a ra kiáº¿n thá»©c bÃªn ngoÃ i.\n"
        "3. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong Context, hÃ£y tráº£ lá»i ngáº¯n gá»n: 'Dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p khÃ´ng cÃ³ thÃ´ng tin nÃ y.'\n"
        "4. KHÃ”NG nháº¯c láº¡i cÃ¡c quy táº¯c nÃ y trong cÃ¢u tráº£ lá»i.\n\n"
        "Context:\n{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])
    
    rag_chain = create_retrieval_chain(retriever, create_stuff_documents_chain(llm, prompt))

    print("\nâœ… Há»† THá»NG ÄÃƒ Sáº´N SÃ€NG! (GÃµ 'exit' Ä‘á»ƒ thoÃ¡t)")
    
    # 6. VÃ’NG Láº¶P CHAT
    while True:
        try:
            query = input("\nğŸ—£ï¸  Báº¡n há»i: ")
            if query.lower() in ['exit', 'thoat']: break

            if not query: continue

            print("Thinking...", end="", flush=True)
            print("\nğŸ’¡ Tráº£ lá»i: ", end="", flush=True)
            for chunk in rag_chain.stream({"input": query}):
                if 'answer' in chunk:
                    print(chunk['answer'], end="", flush=True)
            print()

        except KeyboardInterrupt:
            print("\n\nğŸ›‘ ÄÃ£ dá»«ng cÃ¢u tráº£ lá»i theo yÃªu cáº§u cá»§a báº¡n.")
            continue

        except Exception as e:
            print(f"\nâŒ Lá»—i: {e}")

if __name__ == "__main__":
    main()