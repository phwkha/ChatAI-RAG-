import os
import time

# Láº¥y Ä‘á»‹a chá»‰ cá»§a mÃ¡y AI tá»« file docker-compose
# Náº¿u khÃ´ng tháº¥y thÃ¬ máº·c Ä‘á»‹nh lÃ  localhost
OLLAMA_URL = os.getenv("OLLAMA_HOST", "http://localhost:11434")

print(f"ğŸ”Œ Äang káº¿t ná»‘i tá»›i mÃ¡y AI táº¡i: {OLLAMA_URL}")

# Import cÃ¡c cÃ´ng cá»¥
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

def main():
    print("â³ Äang chá» mÃ¡y AI khá»Ÿi Ä‘á»™ng (5 giÃ¢y)...")
    time.sleep(5) 

    # 1. Äá»ŒC FILE
    print("ğŸ“‚ Äang Ä‘á»c tÃ i liá»‡u data.txt...")
    try:
        loader = TextLoader("data.txt", encoding='utf-8')
        docs = loader.load()
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file: {e}")
        return

    # 2. Cáº®T NHá» & MÃƒ HÃ“A
    print("âœ‚ï¸  Äang xá»­ lÃ½ dá»¯ liá»‡u...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    # 3. Táº O Bá»˜ NHá»š VECTOR
    # DÃ¹ng model 'nomic-embed-text' Ä‘á»ƒ hiá»ƒu vÄƒn báº£n
    embeddings = OllamaEmbeddings(
        model="nomic-embed-text",
        base_url=OLLAMA_URL
    )
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    # 4. KHá»I Táº O DEEPSEEK
    # DÃ¹ng model 'deepseek-r1:1.5b' cho nháº¹ mÃ¡y
    print("ğŸ¤– Äang káº¿t ná»‘i vá»›i DeepSeek...")
    llm = ChatOllama(
        model="deepseek-r1:1.5b",
        base_url=OLLAMA_URL,
        temperature=0.3
    )

    # 5. Táº O Há»˜I THOáº I
    system_prompt = (
        "Báº¡n lÃ  má»™t trá»£ lÃ½ AI nghiÃªm tÃºc vÃ  trung thá»±c. "
        "Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i CHá»ˆ dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong pháº§n ngá»¯ cáº£nh (context) bÃªn dÆ°á»›i.\n"
        "QUY Táº®C:\n"
        "1. TUYá»†T Äá»I KHÃ”NG sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i (nhÆ° lá»‹ch sá»­, Ä‘á»‹a lÃ½, code...) náº¿u khÃ´ng cÃ³ trong vÄƒn báº£n.\n"
        "2. Náº¿u thÃ´ng tin khÃ´ng tá»“n táº¡i trong ngá»¯ cáº£nh, hÃ£y tráº£ lá»i chÃ­nh xÃ¡c cÃ¢u nÃ y: 'Xin lá»—i, dá»¯ liá»‡u cá»§a tÃ´i khÃ´ng cÃ³ thÃ´ng tin nÃ y.'\n\n"
        "Ngá»¯ cáº£nh:\n{context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])
    
    rag_chain = create_retrieval_chain(retriever, create_stuff_documents_chain(llm, prompt))

    print("\nâœ… Há»† THá»NG ÄÃƒ Sáº´N SÃ€NG! (GÃµ 'exit' Ä‘á»ƒ thoÃ¡t)")
    
    # 6. VÃ’NG Láº¶P CHAT
    while True:
        try:
            query = input("\nğŸ—£ï¸  Báº¡n há»i: ")
            if query.lower() in ['exit', 'thoat']: break
            if not query: continue
            print("Thinking...", end="", flush=True)
            result = rag_chain.invoke({"input": query})
            print(f"\nğŸ’¡ Tráº£ lá»i: {result['answer']}")
        except Exception as e:
            print(f"\nâŒ Lá»—i: {e}")

if __name__ == "__main__":
    main()